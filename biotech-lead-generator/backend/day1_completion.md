# ğŸ“… DAY 1 COMPLETION - Database Models & Migration

## âœ… What You've Created Today

You now have **5 complete database models**:

1. âœ… **User Model** - Authentication & subscriptions
2. âœ… **Lead Model** - Core entity with scoring
3. âœ… **Search Model** - Search history tracking
4. âœ… **Export Model** - Export management
5. âœ… **Pipeline Model** - Automated workflows

---

## ğŸ¯ NEXT STEPS: Setup Database Migration

### Step 1: Configure Alembic

Create `alembic.ini` in your `backend/` directory:

```ini
# alembic.ini

[alembic]
# Path to migration scripts
script_location = alembic

# Template used to generate migration files
file_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s

# Timezone for migration timestamps
timezone = UTC

# Maximum length of revision
truncate_slug_length = 40

# Set to 'true' to enable sqlalchemy-migrate mode
sqlalchemy.url = 

[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
```

### Step 2: Configure Alembic env.py

Edit `alembic/env.py`:

```python
"""
Alembic migration environment configuration
"""

from logging.config import fileConfig
from sqlalchemy import engine_from_config, pool
from alembic import context
import sys
import os

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

# Import your app config and models
from app.core.config import settings, get_database_url
from app.core.database import Base

# Import all models so Alembic can detect them
from app.models import (
    User, Lead, Search, Export, Pipeline
)

# Alembic Config object
config = context.config

# Interpret the config file for Python logging
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# Set target metadata for autogenerate
target_metadata = Base.metadata

# Override sqlalchemy.url with our config
config.set_main_option("sqlalchemy.url", get_database_url())


def run_migrations_offline() -> None:
    """
    Run migrations in 'offline' mode.
    
    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.
    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
        compare_type=True,
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """
    Run migrations in 'online' mode.
    
    In this scenario we need to create an Engine
    and associate a connection with the context.
    """
    connectable = engine_from_config(
        config.get_section(config.config_ini_section),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata,
            compare_type=True,
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
```

### Step 3: Create Initial Migration

```bash
# Make sure you're in the backend/ directory
cd backend

# Activate virtual environment
source venv/bin/activate  # Windows: venv\Scripts\activate

# Create initial migration
alembic revision --autogenerate -m "Initial schema with users, leads, searches, exports, pipelines"
```

This will create a file like: `alembic/versions/2024_12_30_1430-abc123_initial_schema.py`

### Step 4: Review Migration File

Open the generated migration file and review it:

```python
# alembic/versions/2024_12_30_1430-abc123_initial_schema.py

"""Initial schema with users, leads, searches, exports, pipelines

Revision ID: abc123
Revises: 
Create Date: 2024-12-30 14:30:00.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers
revision = 'abc123'
down_revision = None
branch_labels = None
depends_on = None


def upgrade() -> None:
    # ### commands auto generated by Alembic ###
    
    # Create users table
    op.create_table('users',
        sa.Column('id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('email', sa.String(length=255), nullable=False),
        sa.Column('password_hash', sa.String(length=255), nullable=False),
        sa.Column('full_name', sa.String(length=255), nullable=True),
        sa.Column('subscription_tier', sa.Enum('FREE', 'PRO', 'TEAM', 'ENTERPRISE', name='subscriptiontier'), nullable=False),
        sa.Column('is_active', sa.Boolean(), nullable=False),
        sa.Column('is_verified', sa.Boolean(), nullable=False),
        sa.Column('is_superuser', sa.Boolean(), nullable=False),
        sa.Column('api_keys', postgresql.JSONB(astext_type=sa.Text()), nullable=False),
        sa.Column('usage_stats', postgresql.JSONB(astext_type=sa.Text()), nullable=False),
        sa.Column('preferences', postgresql.JSONB(astext_type=sa.Text()), nullable=False),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
        sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
        sa.Column('last_login_at', sa.DateTime(timezone=True), nullable=True),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_users_email'), 'users', ['email'], unique=True)
    op.create_index(op.f('ix_users_id'), 'users', ['id'], unique=False)
    
    # Create leads table
    # ... (similar for other tables)
    
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic ###
    
    # Drop all tables in reverse order
    op.drop_table('pipelines')
    op.drop_table('exports')
    op.drop_table('searches')
    op.drop_table('leads')
    op.drop_table('users')
    
    # ### end Alembic commands ###
```

### Step 5: Apply Migration to Database

```bash
# Apply migration to Supabase database
alembic upgrade head
```

You should see output like:
```
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running upgrade  -> abc123, Initial schema
```

### Step 6: Verify in Supabase

1. Go to your Supabase dashboard
2. Click "Table Editor"
3. You should see 5 new tables:
   - `users`
   - `leads`
   - `searches`
   - `exports`
   - `pipelines`
4. Click on each table to see columns

---

## ğŸ§ª TEST YOUR MODELS

Create `scripts/test_models.py`:

```python
"""
Test script to verify models work correctly
"""

import asyncio
from app.core.database import AsyncSessionLocal, init_db
from app.models import User, Lead, SubscriptionTier
from app.core.security import get_password_hash


async def test_models():
    """Test creating and querying models"""
    
    # Initialize database
    await init_db()
    
    # Create session
    async with AsyncSessionLocal() as session:
        try:
            # Create test user
            user = User(
                email="test@example.com",
                password_hash=get_password_hash("Test123!@#"),
                full_name="Test User",
                subscription_tier=SubscriptionTier.FREE,
                is_active=True,
                is_verified=True
            )
            
            session.add(user)
            await session.commit()
            await session.refresh(user)
            
            print(f"âœ… Created user: {user.email} (ID: {user.id})")
            
            # Create test lead
            lead = Lead(
                user_id=user.id,
                name="Dr. Sarah Mitchell",
                title="Director of Toxicology",
                company="Test Biotech",
                location="Cambridge, MA",
                email="sarah@testbiotech.com",
                propensity_score=85
            )
            
            session.add(lead)
            await session.commit()
            await session.refresh(lead)
            
            print(f"âœ… Created lead: {lead.name} (Score: {lead.propensity_score})")
            
            # Test helper methods
            lead.update_priority_tier()
            print(f"âœ… Lead priority tier: {lead.priority_tier}")
            
            lead.add_tag("high-priority")
            lead.add_tag("conference-speaker")
            print(f"âœ… Lead tags: {lead.tags}")
            
            print("\nğŸ‰ All model tests passed!")
            
        except Exception as e:
            print(f"âŒ Error: {e}")
            await session.rollback()
            raise


if __name__ == "__main__":
    asyncio.run(test_models())
```

Run the test:

```bash
python scripts/test_models.py
```

Expected output:
```
âœ… Created user: test@example.com (ID: uuid...)
âœ… Created lead: Dr. Sarah Mitchell (Score: 85)
âœ… Lead priority tier: HIGH
âœ… Lead tags: ['high-priority', 'conference-speaker']

ğŸ‰ All model tests passed!
```

---

## ğŸ“Š VERIFY DATABASE STRUCTURE

### Check Tables in Supabase

```sql
-- Run this query in Supabase SQL editor

-- List all tables
SELECT table_name 
FROM information_schema.tables 
WHERE table_schema = 'public';

-- Check users table structure
SELECT column_name, data_type, is_nullable
FROM information_schema.columns
WHERE table_name = 'users'
ORDER BY ordinal_position;

-- Count records
SELECT 
    'users' as table_name, COUNT(*) as count FROM users
UNION ALL
SELECT 'leads', COUNT(*) FROM leads
UNION ALL
SELECT 'searches', COUNT(*) FROM searches
UNION ALL
SELECT 'exports', COUNT(*) FROM exports
UNION ALL
SELECT 'pipelines', COUNT(*) FROM pipelines;
```

---

## ğŸ¯ DAY 1 CHECKLIST

- [ ] All 5 model files created
- [ ] `app/models/__init__.py` created
- [ ] Alembic configured
- [ ] Initial migration created
- [ ] Migration applied to Supabase
- [ ] Tables visible in Supabase dashboard
- [ ] Test script runs successfully
- [ ] Can create users and leads

---

## ğŸš€ WHAT'S NEXT - DAY 2

Tomorrow we'll build:

1. **Pydantic Schemas** - Request/response validation
   - User schemas (register, login, profile)
   - Lead schemas (create, update, response)
   - Search, Export, Pipeline schemas

2. **Why Schemas?**
   - Validate incoming API requests
   - Serialize outgoing responses
   - Auto-generate API documentation
   - Type safety throughout the app

---

## ğŸ†˜ TROUBLESHOOTING

### Issue: "ModuleNotFoundError: No module named 'app'"

**Solution:**
```bash
# Make sure you're in backend/ directory
cd backend

# Set PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
# Windows: set PYTHONPATH=%PYTHONPATH%;%cd%
```

### Issue: "alembic.util.exc.CommandError: Can't locate revision identified by 'head'"

**Solution:**
```bash
# Delete alembic/versions folder contents
rm alembic/versions/*.py

# Recreate migration
alembic revision --autogenerate -m "Initial schema"
```

### Issue: "Connection refused" to Supabase

**Solution:**
1. Check `.env` file has correct `DATABASE_URL`
2. Verify Supabase project is running
3. Check firewall isn't blocking connection
4. Test connection: `psql $DATABASE_URL`

### Issue: "relation 'users' already exists"

**Solution:**
```bash
# Downgrade migration
alembic downgrade base

# Re-run migration
alembic upgrade head
```

---

## ğŸ“š UNDERSTANDING WHAT YOU BUILT

### Database Relationships

```
User (1) â”€â”€â”€â”€â”€â”€â”€â”€ (Many) Leads
  â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ (Many) Searches
  â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ (Many) Exports
  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ (Many) Pipelines
```

### Key Features

**User Model:**
- Subscription tiers (FREE, PRO, TEAM, ENTERPRISE)
- Usage tracking (leads this month, API calls)
- User preferences (theme, scoring weights)
- API key management

**Lead Model:**
- Complete contact information
- Propensity scoring
- Publication tracking
- Enrichment data storage
- Tags and custom fields

**Search Model:**
- Search history
- Saved searches
- Performance tracking

**Export Model:**
- Export job tracking
- File expiration
- Download counting

**Pipeline Model:**
- Automated workflows
- Scheduling (daily, weekly, monthly)
- Success rate tracking
- Configuration storage

---

## ğŸ“ LEARNING POINTS

### Why PostgreSQL + SQLAlchemy?

- **PostgreSQL:** Powerful relational database with JSONB support
- **SQLAlchemy:** Best Python ORM with async support
- **JSONB columns:** Flexible schema for enrichment data
- **UUIDs:** Better for distributed systems

### Why Alembic?

- **Version control** for database schema
- **Reversible migrations** (upgrade/downgrade)
- **Team collaboration** on database changes
- **Production-safe** schema updates

### Model Design Patterns Used

1. **Soft deletes:** Keep data, mark as inactive
2. **Timestamps:** Track when records were created/updated
3. **JSONB fields:** Flexible data without schema changes
4. **Helper methods:** Business logic in models
5. **Relationships:** Easy navigation between related data

---

## âœ… YOU'RE READY FOR DAY 2!

Great job! You've built a solid foundation. Tomorrow we'll add the API contracts (schemas) that sit on top of these models.

**Questions before we continue?** Let me know what you'd like to clarify!
